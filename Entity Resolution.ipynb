{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-12T05:10:13.804522300Z",
     "start_time": "2023-08-12T05:10:09.005497200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dedupe\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed93ec60964fdeab",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T05:10:13.822528800Z",
     "start_time": "2023-08-12T05:10:13.807526600Z"
    }
   },
   "outputs": [],
   "source": [
    "# config\n",
    "sheet_name = [\"AllMasterDb\", \"Top6KMedsFromMasterDB\", \"DistributorMahaveer\", \"DistributorFMPL_unnati\", \"DistributorVMPL_unnati\", \"DistributorParshva\"]\n",
    "columns = {\n",
    "    \"AllMasterDb\": [\"name\",\"Id\",\"manufacturers\",\"salt_composition\"], \n",
    "    \"Top6KMedsFromMasterDB\": [\"name\"], \n",
    "    \"DistributorMahaveer\": [\"ITEM CODE\", \"ITEM NAME\"],\n",
    "    \"DistributorFMPL_unnati\": [\"Code\",\"Item Name\"], \n",
    "    \"DistributorVMPL_unnati\": [\"Code\",\"Item Name\"],\n",
    "    \"DistributorParshva\": [\"Code\",\"Descreption\"]\n",
    "}\n",
    "\n",
    "standardized_columns = {\n",
    "    \"AllMasterDb\": [\"name\",\"Id\",\"manufacturers\",\"salt_composition\"], \n",
    "    \"Top6KMedsFromMasterDB\": [\"name\"], \n",
    "    \"DistributorMahaveer\": [\"Id\", \"name\"],\n",
    "    \"DistributorFMPL_unnati\": [\"Id\", \"name\"], \n",
    "    \"DistributorVMPL_unnati\": [\"Id\", \"name\"],\n",
    "    \"DistributorParshva\": [\"Id\", \"name\"]\n",
    "}\n",
    "\n",
    "output_columns = {\n",
    "    \"AllMasterDb\": [\"name\",\"Id\",\"manufacturers\",\"salt_composition\"], \n",
    "    \"Top6KMedsFromMasterDB\": [\"dist_name\"], \n",
    "    \"DistributorMahaveer\": [\"Dist_Item_Code\", \"dist_name\"],\n",
    "    \"DistributorFMPL_unnati\": [\"Dist_Item_Code\", \"dist_name\"], \n",
    "    \"DistributorVMPL_unnati\": [\"Dist_Item_Code\", \"dist_name\"],\n",
    "    \"DistributorParshva\": [\"Dist_Item_Code\", \"dist_name\"]\n",
    "}\n",
    "\n",
    "# Define the fields dedupe will pay attention to\n",
    "#\n",
    "# Notice how we are telling dedupe to use a custom field comparator\n",
    "# for the 'Zip' field.\n",
    "fields = [\n",
    "    {'field' : 'name', 'type': 'String'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21924f327b2a021e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T05:10:13.845043700Z",
     "start_time": "2023-08-12T05:10:13.822528800Z"
    }
   },
   "outputs": [],
   "source": [
    "def readData(file_location, sheet_name):\n",
    "    \"\"\"\n",
    "    Read data from excel and create a dictionary of records,\n",
    "    where key is a unique record ID and each column(specified in config) is dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    data_d = {}\n",
    "    # nrows = maximum no of rows to get from excel file\n",
    "    file = pd.read_excel(file_location, sheet_name=sheet_name, nrows=300000)\n",
    "    print(f\"No of Rows in Data is {len(file)}\")\n",
    "    c = columns[sheet_name]\n",
    "    \n",
    "    for i, row in enumerate(file[c].values):\n",
    "        clean_row = [(k, v) for (k, v) in zip(standardized_columns[sheet_name], row)]\n",
    "        data_d[i] = dict(clean_row)\n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8bcbf60d40e2ace",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T06:51:32.227145400Z",
     "start_time": "2023-08-12T06:51:32.212145900Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_output(xml_file, left_sheet, right_sheet, output_file, linked_records)->None:\n",
    "        \n",
    "    left_data = pd.read_excel(xml_file, sheet_name=left_sheet, nrows=300000)[columns[left_sheet]]\n",
    "    left_data.columns = output_columns[left_sheet]\n",
    "    right_data = pd.read_excel(xml_file, sheet_name=right_sheet, nrows=300000)[columns[right_sheet]]\n",
    "    right_data.columns = output_columns[right_sheet]\n",
    "    \n",
    "    left_idx = []\n",
    "    right_idx = []\n",
    "    similarity = []\n",
    "    for(left, right), sim in linked_records:\n",
    "        left_idx.append(left)\n",
    "        right_idx.append(right)\n",
    "        similarity.append(sim)\n",
    "    \n",
    "    dataframe = left_data.iloc[left_idx].to_dict(\"list\")\n",
    "    dataframe.update(right_data.iloc[right_idx].to_dict(\"list\"))\n",
    "    dataframe[\"resemblance\"] = similarity\n",
    "    \n",
    "    df = pd.DataFrame(data = dataframe)\n",
    "    \n",
    "    df = df.sort_values('resemblance', ascending=False).drop_duplicates(['dist_name'])\n",
    "\n",
    "    df.to_excel(output_file, sheet_name=right_sheet, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64ded2309b89c7ab",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T06:51:33.311554100Z",
     "start_time": "2023-08-12T06:51:33.303555300Z"
    }
   },
   "outputs": [],
   "source": [
    "def TrainingDedupe(left_sheet, right_sheet):\n",
    "    \n",
    "    output_file = f'Temp/{left_sheet}_{right_sheet}_output.xlsx'\n",
    "    settings_file = 'Temp/csv_example_learned_settings'\n",
    "    training_file = 'Temp/csv_example_training.json'\n",
    "    xml_file = \"Data/ProductListMasterDB&Distributors.xlsx\"\n",
    "    \n",
    "    print('importing data ...')\n",
    "    data_left = readData(xml_file, left_sheet)\n",
    "    data_right = readData(xml_file, right_sheet)\n",
    "\n",
    "    # Training\n",
    "\n",
    "    if os.path.exists(settings_file):\n",
    "        print('reading from', settings_file)\n",
    "        with open(settings_file, 'rb') as f:\n",
    "            linker = dedupe.StaticRecordLink(f)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Create a new deduper object and pass our data model to it.\n",
    "        linker = dedupe.RecordLink(fields)\n",
    "\n",
    "        # If we have training data saved from a previous run of dedupe,\n",
    "        # look for it and load it in.\n",
    "        # __Note:__ if you want to train from scratch, delete the training_file\n",
    "        if os.path.exists(training_file):\n",
    "            print('reading labeled examples from ', training_file)\n",
    "            with open(training_file, 'rb') as f:\n",
    "                linker.prepare_training(data_left, data_right, f, sample_size=15000)\n",
    "        else:\n",
    "            linker.prepare_training(data_left, data_right, sample_size=15000)\n",
    "\n",
    "\n",
    "        # ## Active learning\n",
    "        # Dedupe will find the next pair of records\n",
    "        # it is least certain about and ask you to label them as duplicates\n",
    "        # or not.\n",
    "        # use 'y', 'n' and 'u' keys to flag duplicates\n",
    "        # press 'f' when you are finished\n",
    "        print('starting active labeling...')\n",
    "\n",
    "        dedupe.console_label(linker)\n",
    "\n",
    "        linker.train()\n",
    "\n",
    "        # When finished, save our training away to disk\n",
    "        with open(training_file, 'w') as tf :\n",
    "            linker.write_training(tf)\n",
    "\n",
    "        # Save our weights and predicates to disk.  If the settings file\n",
    "        # exists, we will skip all the training and learning next time we run\n",
    "        # this file.\n",
    "        with open(settings_file, 'wb') as sf :\n",
    "            linker.write_settings(sf)\n",
    "\n",
    "    \n",
    "    # Clustering\n",
    "\n",
    "    # `partition` will return sets of records that dedupe\n",
    "    # believes are all referring to the same entity.\n",
    "\n",
    "    print('clustering...')\n",
    "    linked_records = linker.join(data_left, data_right, 0.0, constraint= \"many-to-many\")\n",
    "    print('# Matched sets', len(linked_records))\n",
    "    \n",
    "    # Writing Results\n",
    "    create_output(xml_file, left_sheet, right_sheet, output_file, linked_records)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9626b8745be8f919",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-12T06:53:03.088941600Z",
     "start_time": "2023-08-12T06:51:34.272401300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing data ...\n",
      "No of Rows in Data is 283734\n",
      "No of Rows in Data is 10438\n",
      "reading from Temp/csv_example_learned_settings\n",
      "clustering...\n",
      "# Matched sets 96498\n"
     ]
    }
   ],
   "source": [
    "TrainingDedupe(sheet_name[0], sheet_name[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
